Code Walkthrough:
1. Set up environment/imports for your training environment for interacting with the Super Mario Bros video game using Gym framework and NES emulation in Python. It defines actions for Mario's movement as well. 
	a. gym_super_mario_bros: the gym interface for the Super Mario Bros environment. Allows interaction and creation with Mario game environments.
	b. nes_py.wrappers import JoypadSpace: this wrapper is used to map discrete action spaces (i.e. pressing buttons on a gamepad) to the NES emulator's input. 
	c. gym_super_mario_bros.actions import SIMPLE_MOVEMENT: imports a set of predefined actions (button presses) for controlling Mario within the Gym environment. It represents a simplified set of actions for Mario's movement.
	d. os.environ['KMP_DUPLICATE_LIB_OK'] = 'True': sets KMP_DUPLICATE_LIB_OK to True. This variable is relatd to the Intel MAth Kernel Library and setting it to True suppresses warnings about duplicate libraries when using MKL. 

2. We then set up a gym environment for the Super Mario Bros video game and map simplified set of predefined actions to control Mario within the environment. To do this, we set up a loop to interact with the Gym environment, and have random actions be taken in each iteration. Once the game is starts, it resets the environment and renders the game screen. Afterwards, the loop runs for some amount of time and then closes the environment. 
	a. env = gym_super_mario_bros.make('SuperMarioBros-v0'): creates the Gym environment for the SMB game and type of SMB identifier (game version) you want to emulate.
	b. env = JoypadSpace(env, SIMPLE_MOVEMENT): Wraps the previously created gym environment using the JoypadSpace wrapper and specifies the predefined set of actions 'Simple_movement' to control mario. I assume this applies the 'Simple_movement' controls to be applicable to the new environment by wrapping it on env, making it possible to control Mario within the Gym environment. 
3. To test out the environment, we create a simple loop for interacting with a Gym environment in order to play Super Mario Bros.  
	a. done = True: used as a flag to indicate whether the game is over or not. 
	b. for step in range(100000): starts a loop that executes 100,000 iterations. Represents the main game loop where it takes actions into the game. 
	c. Create a if statement to check if the done flag is True. If so, that means the game environment will start up. Stops when done = True. 
	d. If we bypass the done = True if statement, we reset the Gym environment to its initial state so we can play a new game. 
	e. state, reward, done, info = env.step(env.action_space.sample()): In each loop iteration, this line performs a random action by sampling  from the action space sample function of the environment and obtains a state, reward, a new "done" flag, and info as results. 
	f. env.render(): The line renders a current state of the environment aka displaying the game screen and allows you to see the game being played. 
	g. env.close(): Is performed when the loop completes. Closes the Gym environment and cleans up the resources associated with it. 
4. To continue with the project we then install pytorch and torchvision. You can use the CPU version or gpu+cuda version. The version I used to match 3.7.11 is 1.10.1, torchvision = 0.11.2, and torchaudio = 0.10. 
5. We then install stable-baseline3[extra] library + extra dependencies (algorithms, environments, or features used for certain functionalities), as it will be used for reinforcement learning for Python. 
6. We then import GrayScaleObservation, VecframeStack, DummyVecEnv, and pyplot. They are necessary tools/wrappers for reinforcement learning, applying grayscale observations/frames, and visualizing the results of frame stacking.

Side note: observations refer to the information that the agent receives from the environment at a given time step, and it provides them about the current state of the environment, which the agent uses to make decisions on how to interact with the environment. A single observation can take various forms depending on the environment and the problem being solved. It can be a A single observation could be a grayscale image frame that represents the current screen or game level or any other data structure that conveys relevant information about the environment's state.

	a. Pyplot is simply used to create plots and visualizations and is needed to visualize frame stacking. 
	b. VecFrameSTack is a wrapper used for stacking multiple consecutive frames to provide information to the AI agent so it can understand motion and changes in the environment.
	c. DummyVecEnv is a wrapper creating a vectorized environment which allows you to run multiple environments parallel. This result speeds up training and data collection. 
	d. Finally, GrayScaleObservation wrapper converts frames from the Gym environment to grayscale and reduces the color information. 
7. Set up the Gym environment for SMB with simplified controls, grayscale observations/frames, vectorization for parallel execution, and frame stacking. Environment is prepared for reinforcement learning/simulation tasks, and the agent can use the processed observations for decision making. 
	a. env = gym_super_mario_bros.make('SuperMarioBros-v0'): creates the base Gym environment for SMB with a specified version in order to work with the game. 
	b. The environment's then wraps the base environment using the JoypadSpace wrapper and simplfiies controls using predefined SIMPLE_MOVEMENT set of actions to control Mario. It maps the actions to the NES input. 
	c. The environment is then wrapped in a GrayScaleObservation wrapper where it converts the frames to the grayscale. With keep_dim=True, it suggests the grayscale frames will retain some dimensions as the original frames.
	d. Next, with env = DummyVecEnv([lambda: env]), the environment is wrapped inside a vectorized environment created by DummyVEcEnv, which allows you to run multiple in parallel for training purposes. 
	e. We then use env = VecFrameStack(env, 4, channels_order='last') to wrap the environment with the VecFrameStack wrapper, which stacks the last four frames together and creates a single observation that contains a sequence of frames for information for the agent to understand motion and changes that happen over time. 
8. After we are done, we reset the environment to its initial state and retrieves the intial observation (state) of the environment, bringing it back to the starting configurstion of the game. 
9. After we restart the environment, we are taking a action in this environment and it is represented as a list with a single element [5]. The meaning of action values vary depending on the specific environment and how it's configured. 

Side Note: The 5 in env.step([5]) represents an action that the agent is taking within the environment, but for RL with Gym environments, actions are represented as discrete numeric values/vectors, and each one represents a specific action that the agent can perform. In this case, the value 5 represents the "jump" action. 

	a. state: After an action is executed, the new observation/state of the game is held in this variable. 
	b. reward: Holds the variable obtained from the environment as a result of an action is taken. It is a scalar value that provides feedback on the quality of the taken action.
	c. done: A boolean flag that indicates whether the game ended. True if game is over or False if it's continuing.  
	d. info: Contains additional information/metadata about the environment/steps taken. Contents vary based on specific environment. 
10. The code generates a figure with 4 subplots (submaps), with each one displaying a image frame from the 'state' variable in order to visualize the frames the agent sees within the environment as it interacts with it. 
	a. for idx in range(state.shape[3]): represents a loop that iterates over the dimension of the fourth axis of the state variable since state is a 4 dimensional array. It represents the number of frames stacked together in a single observation. 
		- since state is a 4 dimensional array, what range(state.shape[3]) does is iterate from 0 to the size from the fourth dimension of the array and create a sequence of integers representing the indexes of the state array's fourth dimension. 
		- Overall, since state is a multidimensional array containing stacks of image frames, the loop is used to iterate over those frames one by one as it processes and displays one of the frames in order to visualize the individual frame. 
	b. plt.subplot(1, 4, idx + 1): A subplot is set up (1 row of subplots, 4 columns of subplots, and idx+1 parameter indicates the position of the current subplot within the grid) for each loop iteration. 
	c. plt.imshow(state[0][:, :, idx]): Inside the subplot, a image is displayed as it takes a slice of the state variable to access the image data for the current frame specified by idx. 
	d. Afterwards, the matplotlib figure shows all the subplots and the image of the frames from the state array after the loop completes. 
11. We import different libraries and modules for file path management, reinforcement learning algorithms, and model checkpointing to save models during training. 
    a. importing os allows us to use it for path management with files/directories and interaction. 
    b. importing PPO (Proximal Policy Optimization) algorithm from Stable Baselines3 library helps utilize popular reinformcement learning algorithms for training agents within environments. 
    c. importing BaseCallback from stable_baselines3.common.callbacks is a base class for creating custom callbacks during training, in this case it is saving models or it could be logging training progress.
12. We define a custom callback class named TrainAndLoggingCallback that inherits from the BaseCallback class. The functionality is used to save the model during specific intervals during training to ensure that checkpoints are stored in the specified directory "save_path" and resumes training from the saved checkpoint.
    a. def __init__(self, check_freq, save_path, verbose=1): Constructor function and takes in check_freq (integer representing the frequency of how often the callback performs its actions (i.e. how often it should save the model)), save_path (the directory path the model checkpoints are saved in), and verbose (optional integer parameter (default being 1) that controls the level of detail/how much info about the log messages of the callback's actions is printed or logged during the training process aka how detailed the log messages are for the callback's action during training)
        - Vebosity conditions:
            + 0 --> callback is silent and no log messages are generated.
            + 1 (default) --> callback may print/log essential information/progress updates i.e. when model is saved. 
            + >= 2 --> callback provides extensive log messages which are useed for debugging/monitoring the training process closely. 
    b. def _init_callback(self): Called during initalization of the callback, as it creates a directory specified by save_path if the directory doesn't exist already. 
    c. def _on_step(self): After each training step, it checks if the current step number (n_calls) is a multiple of check_freq and if so, the callback saves the model to a file in save_path. Next, the saved model is named with the pattern best_model_{n_calls} where n_calls is the current step number. 
13. We then create individual, defined directory paths. Checkpoint_dir specifies the directory path of where the model checkpoints are saved during training, which is './train/' and is within the current directory of the whole project. The same thing is for Logs_dir, which specifies where the log files/logs (i.e. rewards, episode lengths, and other metrics) related to the training process are stored. It is set for within the current directory and into ./logs/.
14. We created a callback instance from the custom TrainingAndLoggingCalback callback class and assigning it to callback and passed the appropriate parameters in order to save the model checkpoints each specific amount of frequency steps and to save it at the Checkpoint_dir path. 
	a. TrainAndLoggingCallback is the custom callback class you defined earlier in the code, and it inherits from the BaseCallback class and is used for performing actions during training for reinforcement learning models. 
	b. check_freq=10000: specifies the frequency at which the callback should perform its actions aka saving the model every 10,000 steps and putting it into the save_path=Checkpoint_dor path. 
15. We then initialize a PPO reinforcement learning model with specified settings of the policy architecture, gym environment we're using to train the model, verbosity level, log directory specified within tensorboad_log, learning rate, and data collection steps. The model variable represents the PPO model created which is trained and evaulated on the gym environment we specified.
	a. CnnPolicy: the type of policy network architecture specified and used for the PPO algorithm, indicates convolutional neural network policy which is used for processing image-based observations. The policy helps handle CNN inputs.
		- Well suited for environments where observations are images/frames i.e. video game screens or camera feeds. Effective at extracting features from raw pixel data. 
	b. learning_rate=0.000001: Sets the learning rate for the optimizier used within the PPO algorithm. Controls the step size in the optimization process and affects how quickly the model adapts to the training data. 
		- Represents the rate at which the model's parameters are updated during training i.e. the step size or magnitude of updates made to the model's parameters in response to the training data. In this case, the parameters are updated very slowly and cautiously during training, making tiny adjustments in response to the training data. This helps ensure training stability but may require more training iterations to reach a good solution. 
	c. n_steps=512: Sets the number of steps to collect data for each update of the PPO algorithm. Determines the amount of experience that's collected before performing a policy update. 
		- Specifies the number of time steps/interactions with the environment that will be used to collect data before performing a policy update. 
16. We then use the learn() method to train the PPO model on the specified gym environment for a total of 1,000,000 steps. During training, the TrainAndLoggingCallback callback will be executed based on every 10,000 steps in order to save the model checkpoints and perform other actions defined within callback instance specified at the callback parameter. Remember, the callback instance was a class from the TrainAndLoggingCallback. 
	a. model.save('thisisatestmodel'): optionally, you can delete the callback parameter and save your model this way after the training is completed. 
17. Finally, we then load the model through PPO at the specified path, we can choose any model we want i.e. './train/best_model_1000000' or './train/best_model_30000' or './train/best_model_450000'.
18. After we load the model of our choice, we then restart the Gym environment to its intiial state in order to start the simulation for our agent/model. After we reset it, we initialize the new state of the simulation through a instance called state, which provides the agent with stimulator information it needs to start interacting with the refreshed environment. 
19. After restarting the environment and intializing that the intial observation/state of it towards state, we then start a loop to start the game. A infinite loop is started and continues running until explicitly stopped by the user or a certain condition is met. (Despite the done flag being a signal for whether a new game will start or continue, it does not break the infinite while True loop that constantly simulates the current game. Instead, it refreshes a new state of the game. Then continues playing that infinitely till the agent dies or completes the stage. It does not stop the emulation of the game, just the state of the game.)
	a. action, _ = model.predict(state): In each iteration of the loop this line uses the trained model of our choice to predict the next action to take based on the current environment state. 
		- model.predict() takes the current state as a input and returns the selected action i.e. action variable. A placeholder variable, _, is used in this case to store a value that's not needed aka additional information returned from model.predict().
	b. state, reward, done, info = env.step(action): After predicting the action, this line takes the action obtained from before and provides it to the environment using env.step(action). Afterwards, a new state, reward, done flag (aka boolean flag indicating whether the episode is over), and additional info from the environment. 
		- The done variable si a boolean flag that indicates whether the current game ended or not. False - Game is still going, True - Game has stopped i.e. character loses its life or finishes a level. If true, the agent resets the environment and starts a new game. 
	c. env.render(): We then display the current game environment/screen to visualize the action between the agent and the environment. 

20. We can then close out the game but in order to close it fully and properly, we have to use env.close()
